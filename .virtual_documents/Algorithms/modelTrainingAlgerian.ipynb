


import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
import seaborn as sns


df=pd.read_csv('Algerian_forest_cleaned.csv')
df


df.columns


df.head()


# drop day month and year since they're not required
df=df.drop(['day', 'month' , 'year'], axis=1)


df.head()


df['Classes'].value_counts()


df['Classes']=np.where(df['Classes'].str.contains('not fire'),0,1)


df.tail()


df['Classes'].value_counts()



# independent and dependent features
# take any numerical feature for regression problem
x=df.drop('FWI', axis=1)
y=df['FWI']


x.head()


y


from sklearn.model_selection import train_test_split
x_train ,x_test , y_train , y_test= train_test_split(x,y, test_size=0.35, random_state=33)




x_train.shape , x_test.shape


x_train.corr()





# check for multi colinearity
# lets say we have 3 features a, b and c 
# if a is highly corr to b and b is highly corr to c
# then we can drop a feature 


plt.figure(figsize=(10,7))
sns.heatmap(x_train.corr(), annot=True)


def correaltion(dataset , threshold):
    col_corr=set()
    corr_matrix=dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i,j]) > threshold:
                colname=corr_matrix.columns[i]
                col_corr.add(colname)
    return col_corr
    


# thrshold domain expertise
corr_features= correaltion(x_train , 0.85)
corr_features


# drop features when correlation is more than 85%
x_train.drop(corr_features , axis=1 , inplace=True)
x_test.drop(corr_features , axis=1 , inplace=True)
x_train.shape , x_test.shape





from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
x_train_scaled=scaler.fit_transform(x_train)
x_test_scaled= scaler.fit_transform(x_test)


x_train_scaled


# Box plot to understand effect of standard scalar
plt.subplots(figsize=(15,5))
plt.subplot(1,2,1)
sns.boxplot(data=x_train)
plt.title('Boxplot before scaling')
plt.subplot(1,2,2)
sns.boxplot(data=x_train_scaled)
plt.title('Boxplot after scaling')






from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error , r2_score
lr=LinearRegression()
lr.fit(x_train_scaled, y_train)
y_pred= lr.predict(x_test_scaled)
mae= mean_absolute_error(y_test , y_pred)
score=r2_score(y_test, y_pred)
print('Mean absolute error :', mae)
print('r2 score :', score)
plt.scatter(y_test , y_pred) 
# y_test_reset = y_test.reset_index(drop=True)
# y_pred_reset = pd.Series(y_pred).reset_index(drop=True)
# sorted_indices = np.argsort(y_test)
# plt.plot(y_test[sorted_indices], y_pred[sorted_indices], color='red', label='Best Fit Line')
# plt.plot(y_test, y_pred, color='red' )
# plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()])



lr.coef_





from sklearn.linear_model import Lasso
lasso=Lasso()
lasso.fit(x_train_scaled,y_train )
y_pred=lasso.predict(x_test_scaled)
mae=mean_absolute_error(y_test , y_pred)
score=r2_score(y_pred , y_test)
print('Mean absolute error :', mae)
print('r2 score :', score)
plt.scatter(y_test, y_pred)





from sklearn.linear_model import LassoCV
lscv=LassoCV(cv=5)
lscv.fit(x_train_scaled, y_train)




lscv.alpha_


lscv.alphas_


y_pred= lscv.predict(x_test_scaled)
plt.scatter(y_test , y_pred)
mae=mean_absolute_error(y_test , y_pred)
score=r2_score(y_test , y_pred) 
print('Mean absolute error :', mae)
print('r2 score :', score)





from sklearn.linear_model import Ridge
ridge=Ridge() 
ridge.fit(x_train_scaled, y_train) 
y_pred=ridge.predict(x_test_scaled)
mae=mean_absolute_error(y_test , y_pred)
score=r2_score(y_test , y_pred) 
print('Mean absolute error :', mae)
print('r2 score :', score)
plt.scatter(y_test , y_pred)


from sklearn.linear_model import RidgeCV
rcv=RidgeCV(cv=5)
rcv.fit(x_train_scaled, y_train)
y_pred=rcv.predict(x_test_scaled)
mae=mean_absolute_error(y_test , y_pred)
score=r2_score(y_test , y_pred) 
print('Mean absolute error :', mae)
print('r2 score :', score)
plt.scatter(y_test , y_pred)


rcv.get_params()





from sklearn.linear_model import ElasticNet
enet=ElasticNet()
enet.fit(x_train_scaled, y_train)
y_pred=enet.predict(x_test_scaled)
mae=mean_absolute_error(y_test , y_pred)
score=r2_score(y_test , y_pred) 
print('Mean absolute error :', mae)
print('r2 score :', score)
plt.scatter(y_test , y_pred)


from sklearn.linear_model import ElasticNetCV
enetcv=ElasticNet()
enetcv.fit(x_train_scaled, y_train)
y_pred=enetcv.predict(x_test_scaled)
mae=mean_absolute_error(y_test , y_pred)
score=r2_score(y_test , y_pred) 
print('Mean absolute error :', mae)
print('r2 score :', score)
plt.scatter(y_test , y_pred)


# Pickling
import pickle
pickle.dump(scaler , open('scaler.pkl', 'wb'))
pickle.dump(ridge , open('ridge.pkl', 'wb'))



